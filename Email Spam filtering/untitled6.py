# -*- coding: utf-8 -*-
"""Untitled6.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1D1vrojbQE3TBNyO4ue3fpqS-C3NjhEHh
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, mean_absolute_error, mean_squared_error
from collections import Counter

# Load the dataset
spam_data = pd.read_csv('spam.csv')
print(spam_data)

# Convert 'spam' and 'ham' to binary labels
spam_data['spam'] = spam_data['Category'].map({'spam': 1, 'ham': 0})
print(spam_data)

# Visualize the distribution of spam vs. ham messages
spam_data['Category'].value_counts().plot(kind='bar', color=['green', 'red'])
plt.title('Distribution of Spam vs. Ham Messages')
plt.xlabel('Category')
plt.ylabel('Count')
plt.show()

# Identify the most frequent words in both spam and ham messages
spam_words = ' '.join(spam_data[spam_data['Category'] == 'spam']['Message'])
ham_words = ' '.join(spam_data[spam_data['Category'] == 'ham']['Message'])
spam_word_count = Counter(spam_words.split()).most_common(10)
ham_word_count = Counter(ham_words.split()).most_common(10)

# Plotting top words for spam and ham
def plot_top_words(word_count, title):
    words, counts = zip(*word_count)
    plt.figure(figsize=(10,5))
    plt.bar(words, counts, color='skyblue')
    plt.title(title)
    plt.xticks(rotation=45)
    plt.ylabel('Frequency')
    plt.show()

plot_top_words(spam_word_count, 'Top 10 Words in Spam Messages')
plot_top_words(ham_word_count, 'Top 10 Words in Ham Messages')

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(spam_data['Message'], spam_data['label'], test_size=0.2, random_state=42)

# Convert text data into a matrix of token counts using CountVectorizer
vectorizer = CountVectorizer()
X_train_transformed = vectorizer.fit_transform(X_train)
X_test_transformed = vectorizer.transform(X_test)

# Train the Multinomial Naive Bayes model
mnb_model = MultinomialNB()
mnb_model.fit(X_train_transformed, y_train)
mnb_predictions = mnb_model.predict(X_test_transformed)

# Evaluate the Multinomial Naive Bayes model
mnb_accuracy = accuracy_score(y_test, mnb_predictions)
mnb_mae = mean_absolute_error(y_test, mnb_predictions)
mnb_mse = mean_squared_error(y_test, mnb_predictions)
mnb_rmse = np.sqrt(mnb_mse)

# Train the Logistic Regression model
lr_model = LogisticRegression(max_iter=1000)
lr_model.fit(X_train_transformed, y_train)
lr_predictions = lr_model.predict(X_test_transformed)

# Evaluate the Logistic Regression model
lr_accuracy = accuracy_score(y_test, lr_predictions)
lr_mae = mean_absolute_error(y_test, lr_predictions)
lr_mse = mean_squared_error(y_test, lr_predictions)
lr_rmse = np.sqrt(lr_mse)

# Print the results
print("Multinomial Naive Bayes Results:")
print(f"Accuracy: {mnb_accuracy*100:.2f}")
print(f"MAE: {mnb_mae:.4f}")
print(f"MSE: {mnb_mse:.4f}")
print(f"RMSE: {mnb_rmse:.4f}")

print("\nLogistic Regression Results:")
print(f"Accuracy: {lr_accuracy*100:.2f}")
print(f"MAE: {lr_mae:.4f}")
print(f"MSE: {lr_mse:.4f}")
print(f"RMSE: {lr_rmse:.4f}")